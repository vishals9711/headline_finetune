{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'final_headline_train_12000.csv', 'validation': 'final_headline_valid_1200.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/valurank/News_headlines/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The logo of cryptocurrency exchange Binance di...</td>\n",
       "      <td>Binance pauses bitcoin withdrawals due to a 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Police officers, some in riot gear, guard a gr...</td>\n",
       "      <td>White nationalist group members face riot-plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A woman walks past a row of cash machines outs...</td>\n",
       "      <td>Lloyds to give staff 1,000 pounds to ease cost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Amazon logo is seen outside its JFK8 distr...</td>\n",
       "      <td>Amazon offers to share data, boost rivals to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An unexploded shell from a multiple rocket lau...</td>\n",
       "      <td>Both sides using heavier weapons in war in Ukr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12240</th>\n",
       "      <td>FILE PHOTO - Prime Minister of Japan Fumio Kis...</td>\n",
       "      <td>Japan PM Kishida to reshuffle cabinet as COVID...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12241</th>\n",
       "      <td>A TV screen shows that China's People's Libera...</td>\n",
       "      <td>Scrapping U.S.-China military talks deepens ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12242</th>\n",
       "      <td>U.S. Deputy Secretary of State Wendy Sherman s...</td>\n",
       "      <td>Tonga, not China, must decide its future, says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12243</th>\n",
       "      <td>Register now for FREE unlimited access to Reut...</td>\n",
       "      <td>Taiwan tension underscores importance of Phili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12244</th>\n",
       "      <td>A person walks past in front of the headquarte...</td>\n",
       "      <td>Brazil's Eletrobras brings back former CEO Fer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12245 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "0      The logo of cryptocurrency exchange Binance di...   \n",
       "1      Police officers, some in riot gear, guard a gr...   \n",
       "2      A woman walks past a row of cash machines outs...   \n",
       "3      The Amazon logo is seen outside its JFK8 distr...   \n",
       "4      An unexploded shell from a multiple rocket lau...   \n",
       "...                                                  ...   \n",
       "12240  FILE PHOTO - Prime Minister of Japan Fumio Kis...   \n",
       "12241  A TV screen shows that China's People's Libera...   \n",
       "12242  U.S. Deputy Secretary of State Wendy Sherman s...   \n",
       "12243  Register now for FREE unlimited access to Reut...   \n",
       "12244  A person walks past in front of the headquarte...   \n",
       "\n",
       "                                                headline  \n",
       "0      Binance pauses bitcoin withdrawals due to a 's...  \n",
       "1      White nationalist group members face riot-plan...  \n",
       "2      Lloyds to give staff 1,000 pounds to ease cost...  \n",
       "3      Amazon offers to share data, boost rivals to d...  \n",
       "4      Both sides using heavier weapons in war in Ukr...  \n",
       "...                                                  ...  \n",
       "12240  Japan PM Kishida to reshuffle cabinet as COVID...  \n",
       "12241  Scrapping U.S.-China military talks deepens ri...  \n",
       "12242  Tonga, not China, must decide its future, says...  \n",
       "12243  Taiwan tension underscores importance of Phili...  \n",
       "12244  Brazil's Eletrobras brings back former CEO Fer...  \n",
       "\n",
       "[12245 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt format\n",
    "instruction_string = f\"\"\"You are HeadlineGPT, an expert in crafting compelling news headlines. Based on the article provided, generate a concise, attention-grabbing headline that accurately reflects the key points and tone of the story, while also maintaining a clear and engaging writing style.\n",
    "\n",
    "Please respond to the following article.\n",
    "\"\"\"\n",
    "\n",
    "fine_tune_template = lambda article, headline: f'''<s>[INST] {instruction_string} \\n{article} \\n[/INST]\\n''' + headline + \"</s>\"\n",
    "\n",
    "# iterate over the df and create a list of examples\n",
    "fine_tune_example_list = []\n",
    "for i in range(len(df)):\n",
    "    obj = {\"text\":fine_tune_template(df[\"article\"][i],df[\"headline\"][i])}\n",
    "    fine_tune_example_list.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.7.1.tar.gz (126 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Building cuda extension requires PyTorch (>=1.13.0) being installed, please install PyTorch first: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install auto-gptq optimum bitsandbytes\n",
    "%pip uninstall torch -y\n",
    "%pip install torch==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
    "                                             trust_remote_code=False, # prevents running custom model files on your machine\n",
    "                                             revision=\"main\") # which version of model to use in repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # model in training mode (dropout modules are activated)\n",
    "\n",
    "# enable gradient check pointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# enable quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Tokenizer\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"example\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_data = fine_tune_example_list.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "# define training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir= \"shawgpt-ft\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "# train model\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push model to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_name = 'shawhin' # your hf username or org name\n",
    "model_id = hf_name + \"/\" + \"shawgpt-ft\"\n",
    "model.push_to_hub(model_id)\n",
    "trainer.push_to_hub(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
