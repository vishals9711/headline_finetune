{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: auto-gptq in ./venv/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./venv/lib/python3.11/site-packages (from auto-gptq) (0.33.0)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.11/site-packages (from auto-gptq) (2.20.0)\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.11/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from auto-gptq) (1.26.4)\n",
      "Requirement already satisfied: rouge in ./venv/lib/python3.11/site-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: gekko in ./venv/lib/python3.11/site-packages (from auto-gptq) (1.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./venv/lib/python3.11/site-packages (from auto-gptq) (2.1.0)\n",
      "Requirement already satisfied: safetensors in ./venv/lib/python3.11/site-packages (from auto-gptq) (0.4.4)\n",
      "Requirement already satisfied: transformers>=4.31.0 in ./venv/lib/python3.11/site-packages (from auto-gptq) (4.43.4)\n",
      "Requirement already satisfied: peft>=0.5.0 in ./venv/lib/python3.11/site-packages (from auto-gptq) (0.12.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.11/site-packages (from auto-gptq) (4.66.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from accelerate>=0.26.0->auto-gptq) (24.1)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.11/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./venv/lib/python3.11/site-packages (from accelerate>=0.26.0->auto-gptq) (0.24.5)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.11/site-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.6.20)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers>=4.31.0->auto-gptq) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./venv/lib/python3.11/site-packages (from transformers>=4.31.0->auto-gptq) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.11/site-packages (from datasets->auto-gptq) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./venv/lib/python3.11/site-packages (from datasets->auto-gptq) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.11/site-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from datasets->auto-gptq) (2.2.2)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from datasets->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.11/site-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets->auto-gptq) (3.10.2)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.11/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->auto-gptq) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->auto-gptq) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: optimum in ./venv/lib/python3.11/site-packages (1.21.3)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.11/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from optimum) (1.13.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]<4.44.0,>=4.29.0 in ./venv/lib/python3.11/site-packages (from optimum) (4.43.4)\n",
      "Requirement already satisfied: torch>=1.11 in ./venv/lib/python3.11/site-packages (from optimum) (2.1.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from optimum) (24.1)\n",
      "Requirement already satisfied: numpy<2.0 in ./venv/lib/python3.11/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in ./venv/lib/python3.11/site-packages (from optimum) (0.24.5)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.11/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.11/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.6.20)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.11/site-packages (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./venv/lib/python3.11/site-packages (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.11/site-packages (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum) (5.27.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in ./venv/lib/python3.11/site-packages (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.11/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.11/site-packages (from datasets->optimum) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./venv/lib/python3.11/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.11/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.11/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets->optimum) (3.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.11/site-packages (0.43.3)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (from bitsandbytes) (2.1.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.11/site-packages (from torch->bitsandbytes) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: torch 2.1.0\n",
      "Uninstalling torch-2.1.0:\n",
      "  Successfully uninstalled torch-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch==2.1\n",
      "  Using cached torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch==2.1) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch==2.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch==2.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch==2.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch==2.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch==2.1) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch==2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch==2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch==2.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.11/site-packages (from torch==2.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.11/site-packages (from torch==2.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.11/site-packages (from torch==2.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.11/site-packages (from torch==2.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.11/site-packages (from torch==2.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.11/site-packages (from torch==2.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.11/site-packages (from torch==2.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch==2.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.11/site-packages (from torch==2.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch==2.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy->torch==2.1) (1.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[20 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/921416519/tinker/headline_finetune/venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/921416519/tinker/headline_finetune/venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/921416519/tinker/headline_finetune/venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-yfjl5vqz/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 327, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=[])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-yfjl5vqz/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 297, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-yfjl5vqz/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 497, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-yfjl5vqz/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 313, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 21, in <module>\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install auto-gptq\n",
    "%pip install optimum\n",
    "%pip install bitsandbytes\n",
    "%pip uninstall torch -y\n",
    "%pip install torch==2.1\n",
    "%pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# Set the environment variable\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
    "                                             trust_remote_code=True, # prevents running custom model files on your machine\n",
    "                                             revision=\"main\") # which version of model to use in repo\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=False, revision=\"main\")\n",
    "\n",
    "# # Move the entire model to GPU (cuda:0)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.11/site-packages (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"valurank/News_headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt format\n",
    "instruction_string = f\"\"\"You are HeadlineGPT, an expert in crafting compelling news headlines. Based on the article provided, generate a concise, attention-grabbing headline that accurately reflects the key points and tone of the story, while also maintaining a clear and engaging writing style.\n",
    "\n",
    "Please respond to the following article.\n",
    "\"\"\"\n",
    "\n",
    "def fine_tune_template(article, headline):\n",
    "    return f\"<s>[INST] {instruction_string} \\n{article} \\n[/INST]\\n{headline}</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,DataCollatorWithPadding\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # model in training mode (dropout modules are activated)\n",
    "\n",
    "# enable gradient check pointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# enable quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.embed_dropout\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.self_attn.qkv_proj\n",
      "model.layers.0.self_attn.rotary_emb\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.activation_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.resid_attn_dropout\n",
      "model.layers.0.resid_mlp_dropout\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.self_attn.qkv_proj\n",
      "model.layers.1.self_attn.rotary_emb\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.activation_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.resid_attn_dropout\n",
      "model.layers.1.resid_mlp_dropout\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.self_attn.qkv_proj\n",
      "model.layers.2.self_attn.rotary_emb\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.activation_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.resid_attn_dropout\n",
      "model.layers.2.resid_mlp_dropout\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.self_attn.qkv_proj\n",
      "model.layers.3.self_attn.rotary_emb\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.activation_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.resid_attn_dropout\n",
      "model.layers.3.resid_mlp_dropout\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.self_attn.qkv_proj\n",
      "model.layers.4.self_attn.rotary_emb\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.activation_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.resid_attn_dropout\n",
      "model.layers.4.resid_mlp_dropout\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.self_attn.qkv_proj\n",
      "model.layers.5.self_attn.rotary_emb\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.activation_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.resid_attn_dropout\n",
      "model.layers.5.resid_mlp_dropout\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.self_attn.qkv_proj\n",
      "model.layers.6.self_attn.rotary_emb\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.activation_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.resid_attn_dropout\n",
      "model.layers.6.resid_mlp_dropout\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.self_attn.qkv_proj\n",
      "model.layers.7.self_attn.rotary_emb\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.activation_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.resid_attn_dropout\n",
      "model.layers.7.resid_mlp_dropout\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.self_attn.qkv_proj\n",
      "model.layers.8.self_attn.rotary_emb\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.activation_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.resid_attn_dropout\n",
      "model.layers.8.resid_mlp_dropout\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.self_attn.qkv_proj\n",
      "model.layers.9.self_attn.rotary_emb\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.activation_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.resid_attn_dropout\n",
      "model.layers.9.resid_mlp_dropout\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.self_attn.qkv_proj\n",
      "model.layers.10.self_attn.rotary_emb\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.activation_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.resid_attn_dropout\n",
      "model.layers.10.resid_mlp_dropout\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.self_attn.qkv_proj\n",
      "model.layers.11.self_attn.rotary_emb\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.activation_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.resid_attn_dropout\n",
      "model.layers.11.resid_mlp_dropout\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.self_attn.qkv_proj\n",
      "model.layers.12.self_attn.rotary_emb\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.activation_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.resid_attn_dropout\n",
      "model.layers.12.resid_mlp_dropout\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.self_attn.qkv_proj\n",
      "model.layers.13.self_attn.rotary_emb\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.activation_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.resid_attn_dropout\n",
      "model.layers.13.resid_mlp_dropout\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.self_attn.qkv_proj\n",
      "model.layers.14.self_attn.rotary_emb\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.activation_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.resid_attn_dropout\n",
      "model.layers.14.resid_mlp_dropout\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.self_attn.qkv_proj\n",
      "model.layers.15.self_attn.rotary_emb\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.activation_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.resid_attn_dropout\n",
      "model.layers.15.resid_mlp_dropout\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.self_attn.qkv_proj\n",
      "model.layers.16.self_attn.rotary_emb\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.activation_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.resid_attn_dropout\n",
      "model.layers.16.resid_mlp_dropout\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.self_attn.qkv_proj\n",
      "model.layers.17.self_attn.rotary_emb\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.activation_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.resid_attn_dropout\n",
      "model.layers.17.resid_mlp_dropout\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.self_attn.qkv_proj\n",
      "model.layers.18.self_attn.rotary_emb\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.gate_up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.mlp.activation_fn\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.resid_attn_dropout\n",
      "model.layers.18.resid_mlp_dropout\n",
      "model.layers.18.post_attention_layernorm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.self_attn.qkv_proj\n",
      "model.layers.19.self_attn.rotary_emb\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.gate_up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.mlp.activation_fn\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.resid_attn_dropout\n",
      "model.layers.19.resid_mlp_dropout\n",
      "model.layers.19.post_attention_layernorm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.self_attn.qkv_proj\n",
      "model.layers.20.self_attn.rotary_emb\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.gate_up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.mlp.activation_fn\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.resid_attn_dropout\n",
      "model.layers.20.resid_mlp_dropout\n",
      "model.layers.20.post_attention_layernorm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.self_attn.qkv_proj\n",
      "model.layers.21.self_attn.rotary_emb\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.gate_up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.mlp.activation_fn\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.resid_attn_dropout\n",
      "model.layers.21.resid_mlp_dropout\n",
      "model.layers.21.post_attention_layernorm\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.self_attn.qkv_proj\n",
      "model.layers.22.self_attn.rotary_emb\n",
      "model.layers.22.mlp\n",
      "model.layers.22.mlp.gate_up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.22.mlp.activation_fn\n",
      "model.layers.22.input_layernorm\n",
      "model.layers.22.resid_attn_dropout\n",
      "model.layers.22.resid_mlp_dropout\n",
      "model.layers.22.post_attention_layernorm\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.self_attn.qkv_proj\n",
      "model.layers.23.self_attn.rotary_emb\n",
      "model.layers.23.mlp\n",
      "model.layers.23.mlp.gate_up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.23.mlp.activation_fn\n",
      "model.layers.23.input_layernorm\n",
      "model.layers.23.resid_attn_dropout\n",
      "model.layers.23.resid_mlp_dropout\n",
      "model.layers.23.post_attention_layernorm\n",
      "model.layers.24\n",
      "model.layers.24.self_attn\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.self_attn.qkv_proj\n",
      "model.layers.24.self_attn.rotary_emb\n",
      "model.layers.24.mlp\n",
      "model.layers.24.mlp.gate_up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.24.mlp.activation_fn\n",
      "model.layers.24.input_layernorm\n",
      "model.layers.24.resid_attn_dropout\n",
      "model.layers.24.resid_mlp_dropout\n",
      "model.layers.24.post_attention_layernorm\n",
      "model.layers.25\n",
      "model.layers.25.self_attn\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.self_attn.qkv_proj\n",
      "model.layers.25.self_attn.rotary_emb\n",
      "model.layers.25.mlp\n",
      "model.layers.25.mlp.gate_up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.25.mlp.activation_fn\n",
      "model.layers.25.input_layernorm\n",
      "model.layers.25.resid_attn_dropout\n",
      "model.layers.25.resid_mlp_dropout\n",
      "model.layers.25.post_attention_layernorm\n",
      "model.layers.26\n",
      "model.layers.26.self_attn\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.self_attn.qkv_proj\n",
      "model.layers.26.self_attn.rotary_emb\n",
      "model.layers.26.mlp\n",
      "model.layers.26.mlp.gate_up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.26.mlp.activation_fn\n",
      "model.layers.26.input_layernorm\n",
      "model.layers.26.resid_attn_dropout\n",
      "model.layers.26.resid_mlp_dropout\n",
      "model.layers.26.post_attention_layernorm\n",
      "model.layers.27\n",
      "model.layers.27.self_attn\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.self_attn.qkv_proj\n",
      "model.layers.27.self_attn.rotary_emb\n",
      "model.layers.27.mlp\n",
      "model.layers.27.mlp.gate_up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.27.mlp.activation_fn\n",
      "model.layers.27.input_layernorm\n",
      "model.layers.27.resid_attn_dropout\n",
      "model.layers.27.resid_mlp_dropout\n",
      "model.layers.27.post_attention_layernorm\n",
      "model.layers.28\n",
      "model.layers.28.self_attn\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.self_attn.qkv_proj\n",
      "model.layers.28.self_attn.rotary_emb\n",
      "model.layers.28.mlp\n",
      "model.layers.28.mlp.gate_up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.28.mlp.activation_fn\n",
      "model.layers.28.input_layernorm\n",
      "model.layers.28.resid_attn_dropout\n",
      "model.layers.28.resid_mlp_dropout\n",
      "model.layers.28.post_attention_layernorm\n",
      "model.layers.29\n",
      "model.layers.29.self_attn\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.self_attn.qkv_proj\n",
      "model.layers.29.self_attn.rotary_emb\n",
      "model.layers.29.mlp\n",
      "model.layers.29.mlp.gate_up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.29.mlp.activation_fn\n",
      "model.layers.29.input_layernorm\n",
      "model.layers.29.resid_attn_dropout\n",
      "model.layers.29.resid_mlp_dropout\n",
      "model.layers.29.post_attention_layernorm\n",
      "model.layers.30\n",
      "model.layers.30.self_attn\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.self_attn.qkv_proj\n",
      "model.layers.30.self_attn.rotary_emb\n",
      "model.layers.30.mlp\n",
      "model.layers.30.mlp.gate_up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.30.mlp.activation_fn\n",
      "model.layers.30.input_layernorm\n",
      "model.layers.30.resid_attn_dropout\n",
      "model.layers.30.resid_mlp_dropout\n",
      "model.layers.30.post_attention_layernorm\n",
      "model.layers.31\n",
      "model.layers.31.self_attn\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.self_attn.qkv_proj\n",
      "model.layers.31.self_attn.rotary_emb\n",
      "model.layers.31.mlp\n",
      "model.layers.31.mlp.gate_up_proj\n",
      "model.layers.31.mlp.down_proj\n",
      "model.layers.31.mlp.activation_fn\n",
      "model.layers.31.input_layernorm\n",
      "model.layers.31.resid_attn_dropout\n",
      "model.layers.31.resid_mlp_dropout\n",
      "model.layers.31.post_attention_layernorm\n",
      "model.norm\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 3,825,798,144 || trainable%: 0.1233\n"
     ]
    }
   ],
   "source": [
    "# LoRA config\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"self_attn.qkv_proj\", \"self_attn.o_proj\"],  # Adjusted to match your model\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# data collator\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/921416519/tinker/headline_finetune/venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 2\n",
    "num_epochs = 10\n",
    "\n",
    "# define training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir= \"vishals9711_ft\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12245/12245 [00:03<00:00, 3837.06 examples/s]\n",
      "Map: 100%|██████████| 1200/1200 [00:00<00:00, 4390.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the tokenize function\n",
    "def tokenize_function(batch):\n",
    "    # Extract text\n",
    "    articles = batch['article']\n",
    "    headlines = batch['headline']\n",
    "    texts = [fine_tune_template(article=a, headline=h) for a, h in zip(articles, headlines)]\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_data = data.map(tokenize_function, batched=True)\n",
    "# test_tokenized_data = [tokenize_function(item) for item in fine_tune_test_list]\n",
    "# train_tokenized_data = [tokenize_function(item) for item in fine_tune_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# renable warnings\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/accelerate/accelerator.py:2155\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/torch/autograd/function.py:288\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:288\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[0;32m--> 288\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    290\u001b[0m     inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[1;32m    292\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[0;32m~/tinker/headline_finetune/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cuda:0"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "# train model\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push model to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_name = 'shawhin' # your hf username or org name\n",
    "model_id = hf_name + \"/\" + \"shawgpt-ft\"\n",
    "model.push_to_hub(model_id)\n",
    "trainer.push_to_hub(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
